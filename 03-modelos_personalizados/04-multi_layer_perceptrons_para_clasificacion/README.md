# üî¢ MNIST Digit Classifier: Reconocimiento Jer√°rquico con TensorFlow.js

Este proyecto implementa una red neuronal de tipo **Perceptr√≥n Multicapa (MLP)** dise√±ada para la clasificaci√≥n de d√≠gitos escritos a mano. Representa un cambio de paradigma fundamental: en lugar de programar reglas l√≥gicas para detectar cada n√∫mero, el sistema **infiere sus propias reglas** a partir de miles de ejemplos del dataset MNIST, logrando una precisi√≥n superior al **93%** ejecut√°ndose √≠ntegramente en el hardware del cliente.

---

## üß† ¬øQu√© es un Multi-Layer Perceptron (MLP)?

Un MLP es una red neuronal densa que utiliza el **Aprendizaje Jer√°rquico de Patrones**. La magia reside en su capacidad para aprender "patrones de patrones": las primeras capas detectan l√≠neas y bordes, mientras que las capas profundas ensamblan esas formas para reconocer estructuras complejas.

### Componentes de la Arquitectura:

- **Capa de Entrada (Input Layer):** Act√∫a como el receptor del "ADN de los datos". Cada imagen de $28 \times 28$ p√≠xeles se "aplana" en un tensor unidimensional de **784 valores**.
- **Capas Ocultas (Hidden Layers):** Son las "bisagras" matem√°ticas de la red. Utilizan funciones de activaci√≥n para romper la linealidad y permitir que el modelo aprenda la complejidad del mundo real.
- **Capa de Salida (Output Layer):** Proyecta la probabilidad final sobre **10 categor√≠as** (d√≠gitos 0-9).

---

## üèóÔ∏è Anatom√≠a del Modelo y Activaciones

Para garantizar una inferencia fluida en el navegador, el modelo utiliza una estructura optimizada que reside directamente en la **VRAM de la GPU** mediante WebGL:

- **Entrada:** Tensor 1D de 784 valores.
- **Capa Oculta 1:** 32 neuronas con activaci√≥n **ReLU**. La funci√≥n ReLU act√∫a silenciando neuronas con valores negativos, permitiendo un aprendizaje eficiente y r√°pido.
- **Capa Oculta 2:** 16 neuronas con activaci√≥n **ReLU**.
- **Capa de Salida:** 10 neuronas con activaci√≥n **Softmax**.

> **Dato T√©cnico:** La funci√≥n **Softmax** es indispensable para la clasificaci√≥n multiclase; convierte las salidas de la red en una distribuci√≥n de probabilidad donde la suma de todas las opciones es exactamente **1.0 (100%)**.

---

## ‚öôÔ∏è Estrategia de Optimizaci√≥n (Compilaci√≥n)

La compilaci√≥n define la "br√∫jula" que guiar√° al modelo durante el entrenamiento:

```javascript
model.compile({
  optimizer: tf.train.adam(),
  loss: 'categoricalCrossentropy',
  metrics: ['accuracy'],
});
```

- **Optimizador Adam:** Un algoritmo de aprendizaje adaptativo que ajusta la tasa de aprendizaje (_Learning Rate_) din√°micamente. Esto permite dar pasos grandes al principio y pasos m√°s finos al final, evitando que el modelo se estanque en m√≠nimos locales.

- **Loss (Categorical Crossentropy):** La funci√≥n de p√©rdida encargada de medir la distancia matem√°tica entre la predicci√≥n de la red y la "Verdad Fundamental" (_Ground Truth_).
- **Accuracy:** M√©trica humana para monitorear el porcentaje de aciertos en tiempo real sobre el dataset.

---

## ‚ö° El Ciclo de Vida del Dato y Gesti√≥n de Memoria

A diferencia de los objetos est√°ndar de JavaScript, los **Tensores** en TensorFlow.js son inmutables y viven en la memoria de la GPU, lo que requiere una gesti√≥n activa para evitar fugas de memoria (_Memory Leaks_).

- **Normalizaci√≥n:** Los p√≠xeles se escalan del rango $[0, 255]$ al rango $[0, 1]$ mediante una transformaci√≥n **Min-Max**. Esto estabiliza los gradientes y evita que valores de p√≠xeles muy altos saturen las neuronas, facilitando el entrenamiento.
- **One-Hot Encoding:** Las etiquetas (d√≠gitos del 0 al 9) se transforman en vectores binarios. Por ejemplo, el n√∫mero 3 se convierte en $[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]$. Esto permite que la funci√≥n de p√©rdida calcule el error de forma categ√≥rica e independiente para cada clase.

- **Gesti√≥n de Tensores:** \* `tf.tidy()`: Utilidad que limpia autom√°ticamente los tensores intermedios generados en operaciones matem√°ticas complejas.
  - `tf.dispose()`: Liberaci√≥n manual de memoria para tensores de entrada y salida una vez finalizada la sesi√≥n de inferencia.

### Hiperpar√°metros de Entrenamiento

| Par√°metro            | Valor | Impacto T√©cnico                                                          |
| :------------------- | :---- | :----------------------------------------------------------------------- |
| **Batch Size**       | 512   | Equilibrio entre la velocidad de la GPU y la estabilidad del gradiente.  |
| **Epochs**           | 50    | Cantidad de iteraciones completas sobre el dataset (Ciclo de ajuste).    |
| **Validation Split** | 0.2   | Reserva el 20% de los datos para evaluar la capacidad de Generalizaci√≥n. |

---

## üìä Monitoreo y Evaluaci√≥n

El sistema incluye un bucle de evaluaci√≥n continua que demuestra la capacidad de **Inferencia** del modelo en tiempo real:

1. **Extracci√≥n de Muestra:** Se obtiene una imagen aleatoria del set de prueba (datos que el modelo nunca vio en el entrenamiento).
2. **Predicci√≥n:** El modelo ejecuta `model.predict(input)`, devolviendo un tensor con 10 probabilidades (una para cada d√≠gito).
3. **Resultado:** Se utiliza `.argMax()` para identificar el √≠ndice con la probabilidad m√°s alta y se sincroniza con la CPU mediante `.dataSync()` para mostrarlo en pantalla.

### Estado del Sistema:

- üü¢ **Verde:** Predicci√≥n correcta. El modelo ha generalizado bien el patr√≥n visual.
- üî¥ **Rojo:** Error de clasificaci√≥n. Indica l√≠mites en la arquitectura, ambig√ºedad en el trazo del n√∫mero o necesidad de m√°s √©pocas de entrenamiento.
