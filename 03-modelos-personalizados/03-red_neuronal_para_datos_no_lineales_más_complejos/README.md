# üß¨ Proyecto 3: MLP - Rompiendo la Barrera de la Linealidad

Este proyecto representa la evoluci√≥n natural desde el perceptr√≥n simple hacia el **Deep Learning**. Mientras que una sola neurona es f√≠sicamente incapaz de "doblarse", esta **Red Neuronal Multicapa (MLP)** utiliza capas ocultas y funciones de activaci√≥n no lineales para aproximar funciones complejas, logrando aprender con precisi√≥n la curvatura de una par√°bola ($y = x^2$).

---

## üöÄ Caracter√≠sticas Principales

- **Arquitectura Deep Learning:** Implementaci√≥n de una red densa con m√∫ltiples niveles de abstracci√≥n.
- **Activaci√≥n No Lineal:** Uso de **ReLU** para introducir "bisagras" matem√°ticas que permiten al modelo ajustarse a curvas.
- **Visualizaci√≥n Reactiva:** Gr√°fica din√°mica basada en **Chart.js** que renderiza el ajuste del modelo en tiempo real (Real-time Regression).
- **Dise√±o Modular:** Separaci√≥n estricta de responsabilidades bajo una arquitectura de servicios (ML, UI, Plot).
- **UI de Alta Fidelidad:** Interfaz minimalista y responsiva construida con **Tailwind CSS**.

---

## üß† Arquitectura del Modelo

Para superar el subajuste (_underfitting_) de los proyectos anteriores, hemos dise√±ado un aproximador universal con la siguiente configuraci√≥n:

1.  **Capa de Entrada:** 1 neurona (recibe el valor escalar $x$).
2.  **Capa Oculta 1:** 32 neuronas con activaci√≥n **ReLU**.
3.  **Capa Oculta 2:** 16 neuronas con activaci√≥n **ReLU**.
4.  **Capa de Salida:** 1 neurona con activaci√≥n **Linear** para la predicci√≥n del valor continuo $y$.
5.  **Optimizador:** **Adam** ($lr = 0.01$), elegido por su capacidad de ajustar la tasa de aprendizaje de forma adaptativa.
6.  **Funci√≥n de P√©rdida:** Error Cuadr√°tico Medio (**MSE**).

---

## üõ†Ô∏è Conceptos T√©cnicos Fundamentales

### 1. ¬øPor qu√© ReLU? (Rectified Linear Unit)

Sin una funci√≥n de activaci√≥n, el apilamiento de capas es matem√°ticamente equivalente a una sola capa lineal. ReLU act√∫a como una "bisagra" que permite que la red se doble. Al combinar m√∫ltiples neuronas con ReLU, la red crea una aproximaci√≥n por tramos que imita perfectamente una curva suave.

### 2. Normalizaci√≥n Min-Max (Feature Scaling)

El entrenamiento de redes profundas es sensible a la magnitud de los datos. Normalizamos las entradas al rango $[0, 1]$ para garantizar que los gradientes fluyan sin explotar ni desvanecerse.

> **Nota t√©cnica:** Extraemos los valores de normalizaci√≥n como n√∫meros nativos de JavaScript para asegurar que los par√°metros sobrevivan al ciclo de limpieza de memoria de TensorFlow.

### 3. Gesti√≥n de Memoria y Tensores

El proyecto implementa una gesti√≥n de recursos estricta para evitar fugas de memoria en el navegador:

- **`tf.tidy()`:** Limpia autom√°ticamente los tensores intermedios durante el entrenamiento.
- **`.dataSync()`:** Sincroniza los datos de la GPU con la CPU para la visualizaci√≥n final.

---

## üìÇ Estructura del M√≥dulo

```text
03-mlp-cuadratico/
‚îú‚îÄ‚îÄ index.html              # Estructura de la interfaz
‚îú‚îÄ‚îÄ README.md               # Documentaci√≥n t√©cnica
‚îî‚îÄ‚îÄ js/
    ‚îú‚îÄ‚îÄ config.js           # Hiperpar√°metros y Single Source of Truth
    ‚îú‚îÄ‚îÄ main.js             # Orquestador y ciclo de vida de la App
    ‚îî‚îÄ‚îÄ services/
        ‚îú‚îÄ‚îÄ mlService.js    # Motor de TensorFlow.js y gesti√≥n de modelos
        ‚îú‚îÄ‚îÄ uiService.js    # Gesti√≥n del DOM y terminal del sistema
        ‚îî‚îÄ‚îÄ plotService.js  # Pipeline de renderizado de Chart.js
```

## üíª Ejecuci√≥n y Desarrollo

Debido al uso de **m√≥dulos ES6** y la carga de recursos externos (como los pesos del modelo y librer√≠as desde CDN), el navegador bloquea el acceso a archivos locales por pol√≠ticas de seguridad (**CORS**). Por ello, es imperativo ejecutar el proyecto a trav√©s de un servidor web.

### ‚öôÔ∏è Pasos para el Despliegue Local

1.  **Iniciar un Servidor Local:**
    Si tienes **Node.js** instalado, puedes usar `http-server`:

    ```bash
    npx http-server . --cors
    ```

    Alternativamente, puedes usar la extensi√≥n **Live Server** en VS Code.

2.  **Acceso al Laboratorio:**
    Abre tu navegador en `http://localhost:8080` (o el puerto indicado por tu servidor).

### üéÆ Interacci√≥n con el Modelo

- **Entrenamiento:** Haz clic en el bot√≥n **"INICIAR ENTRENAMIENTO"**. Observar√°s c√≥mo la l√≠nea de predicci√≥n comienza como una recta y, gracias a las capas ocultas y la activaci√≥n **ReLU**, empieza a curvarse progresivamente para "abrazar" los puntos de la par√°bola.
- **Monitoreo de Convergencia:** La gr√°fica de p√©rdida (_Loss Curve_) mostrar√° una pendiente descendente, indicando que el optimizador **Adam** est√° encontrando los pesos ideales.

---

## üìà An√°lisis de Resultados: El Poder de la No-Linealidad

A diferencia de los experimentos anteriores, el **MLP** logra:

1.  **Aproximaci√≥n Universal:** La capacidad de modelar la funci√≥n $y = x^2$ con un error residual m√≠nimo.
2.  **Generalizaci√≥n:** Capacidad de predecir valores de $x$ no incluidos en el set de entrenamiento con alta precisi√≥n.
3.  **Estabilidad:** El optimizador Adam evita las oscilaciones bruscas, logrando un entrenamiento fluido incluso con una tasa de aprendizaje alta ($0.01$).

---

> **Conclusi√≥n del M√≥dulo:** Con este proyecto, hemos validado que la arquitectura (neuronas + capas + activaciones) es tan importante como los datos. Hemos pasado de una "regla r√≠gida" a un sistema con "articulaciones" capaz de aprender la complejidad del mundo real.
